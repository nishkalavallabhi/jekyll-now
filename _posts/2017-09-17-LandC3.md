---
layout: post
title: Teaching Notes - Teaching about language tutoring systems
categories: Teaching, LING120
---

This post is a continuation from my previous posts - [Part 1](https://nishkalavallabhi.github.io/LandC1/) [Part 2](https://nishkalavallabhi.github.io/LandC2/) on teaching a 100-level undergraduate course called **Language and Computers**. As mentioned earlier, it is a very diverse class, and I use this textbook: [Language and Computers](http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP002779.html) by Marcus Dickinson, Chris Brew and Detmar Meurers. This post is about Chapter 3 of that book, called Language Tutoring Systems. The chapter primarily dealt with idea of **Computer Assisted Language Learning** i.e., designing software to teach and assess learning of a new language.

It started with an overview of language acquisition, and then, introducing what kind of exercises typical CALL systems have, what kind of language analysis is needed (and why?) etc. Finally, there was one example of a intelligent CALL (ICALL) system called TAGARELA, and a little bit on learner modeling.

I spent a week (3*50 minutes) on the topic, and this is perhaps my strongest deviation/disagreement with this book so far. So much happened in the world of app-based language learning in the past few years that I felt it deserves some mention here - whether there is NLP or not, it is the interaction of language and computers. Further, I felt the role of speech recognition in these systems has been ignored in this book. Most of the responses to my questions (see below) gave me an impression that students assume speech recognition and synthesis as a given in these systems (it is, in some of them). 

So, here is how I went about doing this in the class. After giving a quick overview of what does a tutoring system do, I spent the whole of first class asking questions about:
- What a ICALL system should be able to do
- What level of language analysis is needed to be able to do that
- What are the different forms of questions for practice, and for testing learning?
- How to create some forms of questions (multiple choice, fill in the blanks)
- What are canned responses, what are "generated" responses
- What is frame based system and what is "intelligent"?
- How should feedback be given (and how much)?
It was mostly getting answers, getting students think in this lines kind of class.

In the second class, I spent time discussing some of these questions in detail (questions 2--4 primarily). Finally, the third class was totally dedicated to showing the demos and videos of various tutoring systems, software to practice language structures and feedback systems (e.g., [VIEW](http://sifnos.sfs.uni-tuebingen.de/VIEW/), [Project Listen](http://www.cs.cmu.edu/~./listen/), Grammarly, Mobile based language learning apps such as DuoLingo and Babbel, Projects@ISU such as: CyWrite, GoldenSpeaker, Research Writing Tutor etc.). Through the videos, I tried to talk about different issues involved in these systems, and how they are different from each other etc. 

Next time around, I should perhaps touch upon the following questions too:
- How do we evaluate such systems?
- How do we generate different kinds of questions?
- How do we evaluate different kinds of answers?
(I already dealt with the last two, by asking them to write their thoughts on these as my attendance questions -yes that is how I take attendance). 

While asking these questions, I kept asking myself - "What is the purpose of this class? Do they really need to know answers for these relatively open research problems?" But then, I think part of the idea is to also expose the students to such issues so that they understand the challenges of working on that topic. 

Overall, I felt this came out well - the idea of three classes - one with questions, the other with answers, and the last one with real-world examples. The next topic is search - hopefully I will blog about my experience teaching that in a week or two!


